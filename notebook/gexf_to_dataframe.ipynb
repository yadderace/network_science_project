{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEXF Network to DataFrame Conversion\n",
    "\n",
    "This notebook converts the largest_component.gexf network file to a DataFrame containing node information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GEXF file: ../data/largest_component.gexf\n",
      "Successfully loaded network with 1899 nodes and 2313 edges\n"
     ]
    }
   ],
   "source": [
    "# Load the GEXF file\n",
    "gexf_file = '../data/largest_component.gexf'\n",
    "print(f\"Loading GEXF file: {gexf_file}\")\n",
    "\n",
    "try:\n",
    "    # Read the GEXF file using NetworkX\n",
    "    G = nx.read_gexf(gexf_file)\n",
    "    print(f\"Successfully loaded network with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading GEXF file: {e}\")\n",
    "    G = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Information:\n",
      "Number of nodes: 1899\n",
      "Number of edges: 2313\n",
      "Is directed: True\n",
      "\n",
      "Sample nodes with attributes:\n",
      "Node ID: 526\n",
      "Attributes: {'scientific_name': 'Asimina reticulata', 'common_name': 'netted pawpaw', 'category': 'Plantae', 'label': '526'}\n",
      "\n",
      "Node ID: 4755\n",
      "Attributes: {'scientific_name': 'Thomisus onustus', 'common_name': 'Heather crab spider', 'category': 'Arachnida', 'label': '4755'}\n",
      "\n",
      "Node ID: 3161\n",
      "Attributes: {'scientific_name': 'Nerodia', 'common_name': 'Watersnakes', 'category': 'Reptilia', 'label': '3161'}\n",
      "\n",
      "Node ID: 4493\n",
      "Attributes: {'scientific_name': 'Stagmomantis carolina', 'common_name': 'Carolina Mantis', 'category': 'Insecta', 'label': '4493'}\n",
      "\n",
      "Node ID: 757\n",
      "Attributes: {'scientific_name': 'Bridelia micrantha', 'common_name': 'Coastal Goldenleaf', 'category': 'Plantae', 'label': '757'}\n",
      "\n",
      "Available node attributes: ['category', 'common_name', 'label', 'scientific_name']\n"
     ]
    }
   ],
   "source": [
    "# Explore the network structure and node attributes\n",
    "if G is not None:\n",
    "    print(\"Network Information:\")\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "    print(f\"Is directed: {G.is_directed()}\")\n",
    "    \n",
    "    # Check a few sample nodes and their attributes\n",
    "    sample_nodes = list(G.nodes(data=True))[:5]\n",
    "    print(\"\\nSample nodes with attributes:\")\n",
    "    for node_id, attributes in sample_nodes:\n",
    "        print(f\"Node ID: {node_id}\")\n",
    "        print(f\"Attributes: {attributes}\")\n",
    "        print()\n",
    "    \n",
    "    # Get all unique attribute keys across all nodes\n",
    "    all_attributes = set()\n",
    "    for _, attributes in G.nodes(data=True):\n",
    "        all_attributes.update(attributes.keys())\n",
    "    \n",
    "    print(f\"Available node attributes: {sorted(all_attributes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 1899 records\n",
      "Columns: ['ID', 'scientific_name', 'common_name', 'category']\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1899 entries, 0 to 1898\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   ID               1899 non-null   object\n",
      " 1   scientific_name  1899 non-null   object\n",
      " 2   common_name      1899 non-null   object\n",
      " 3   category         1899 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 59.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Convert network nodes to DataFrame\n",
    "def network_to_dataframe(graph):\n",
    "    \"\"\"\n",
    "    Convert NetworkX graph nodes to pandas DataFrame\n",
    "    \"\"\"\n",
    "    if graph is None:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extract node data\n",
    "    node_data = []\n",
    "    \n",
    "    for node_id, attributes in graph.nodes(data=True):\n",
    "        # Create a record for each node\n",
    "        record = {\n",
    "            'ID': node_id,\n",
    "            'scientific_name': attributes.get('scientific_name', ''),\n",
    "            'common_name': attributes.get('common_name', ''),\n",
    "            'category': attributes.get('category', '')\n",
    "        }\n",
    "        node_data.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(node_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert to DataFrame\n",
    "nodes_df = network_to_dataframe(G)\n",
    "\n",
    "if not nodes_df.empty:\n",
    "    print(f\"Created DataFrame with {len(nodes_df)} records\")\n",
    "    print(f\"Columns: {list(nodes_df.columns)}\")\n",
    "    print(f\"\\nDataFrame info:\")\n",
    "    print(nodes_df.info())\n",
    "else:\n",
    "    print(\"Failed to create DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 records:\n",
      "     ID          scientific_name                  common_name   category\n",
      "0   526       Asimina reticulata                netted pawpaw    Plantae\n",
      "1  4755         Thomisus onustus          Heather crab spider  Arachnida\n",
      "2  3161                  Nerodia                  Watersnakes   Reptilia\n",
      "3  4493    Stagmomantis carolina              Carolina Mantis    Insecta\n",
      "4   757       Bridelia micrantha           Coastal Goldenleaf    Plantae\n",
      "5  1253       Copsychus saularis        Oriental Magpie-Robin       Aves\n",
      "6   702          Bombus pratorum             Early Bumble Bee    Insecta\n",
      "7  4941        Tyrannus tyrannus             Eastern Kingbird       Aves\n",
      "8  1361   Cucullia scrophulariae                 Water Betony    Insecta\n",
      "9  2490  Lamprotornis chalybaeus  Greater Blue-eared Starling       Aves\n",
      "\n",
      "Last 5 records:\n",
      "        ID        scientific_name               common_name        category\n",
      "1894  1791            Esox lucius             Northern Pike  Actinopterygii\n",
      "1895  2940  Metacarcinus magister            Dungeness Crab        Animalia\n",
      "1896   138      Agrius convolvuli      Convolvulus Hawkmoth         Insecta\n",
      "1897  4340         Senegalia afra  Common Hook-thorn Acacia         Plantae\n",
      "1898  4487      Squalus acanthias             Spiny Dogfish        Animalia\n"
     ]
    }
   ],
   "source": [
    "# Display sample data\n",
    "if not nodes_df.empty:\n",
    "    print(\"First 10 records:\")\n",
    "    print(nodes_df.head(10))\n",
    "    \n",
    "    print(\"\\nLast 5 records:\")\n",
    "    print(nodes_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Analysis:\n",
      "========================================\n",
      "Missing values:\n",
      "  ID: 0 (0.0%)\n",
      "  scientific_name: 0 (0.0%)\n",
      "  common_name: 0 (0.0%)\n",
      "  category: 0 (0.0%)\n",
      "\n",
      "Empty string values:\n",
      "  scientific_name: 0 (0.0%)\n",
      "  common_name: 0 (0.0%)\n",
      "  category: 0 (0.0%)\n",
      "\n",
      "Unique categories (11):\n",
      "category\n",
      "Insecta           627\n",
      "Plantae           428\n",
      "Aves              319\n",
      "Arachnida         133\n",
      "Actinopterygii    107\n",
      "Mammalia           90\n",
      "Animalia           70\n",
      "Reptilia           64\n",
      "Amphibia           28\n",
      "Mollusca           23\n",
      "Fungi              10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Duplicate IDs: 0\n",
      "Duplicate scientific names: 0\n"
     ]
    }
   ],
   "source": [
    "# Data quality analysis\n",
    "if not nodes_df.empty:\n",
    "    print(\"Data Quality Analysis:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"Missing values:\")\n",
    "    missing_counts = nodes_df.isnull().sum()\n",
    "    for col, count in missing_counts.items():\n",
    "        print(f\"  {col}: {count} ({count/len(nodes_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for empty strings\n",
    "    print(\"\\nEmpty string values:\")\n",
    "    for col in ['scientific_name', 'common_name', 'category']:\n",
    "        if col in nodes_df.columns:\n",
    "            empty_count = (nodes_df[col] == '').sum()\n",
    "            print(f\"  {col}: {empty_count} ({empty_count/len(nodes_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Show unique categories if available\n",
    "    if 'category' in nodes_df.columns and not nodes_df['category'].empty:\n",
    "        print(f\"\\nUnique categories ({nodes_df['category'].nunique()}):\")\n",
    "        category_counts = nodes_df['category'].value_counts()\n",
    "        print(category_counts)\n",
    "    \n",
    "    # Check for duplicate IDs\n",
    "    duplicate_ids = nodes_df['ID'].duplicated().sum()\n",
    "    print(f\"\\nDuplicate IDs: {duplicate_ids}\")\n",
    "    \n",
    "    # Check for duplicate scientific names\n",
    "    if 'scientific_name' in nodes_df.columns:\n",
    "        duplicate_sci_names = nodes_df['scientific_name'].duplicated().sum()\n",
    "        print(f\"Duplicate scientific names: {duplicate_sci_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning DataFrame...\n",
      "Cleaned DataFrame shape: (1899, 4)\n",
      "\n",
      "Cleaned data sample:\n",
      "     ID        scientific_name          common_name   category\n",
      "0   526     Asimina reticulata        netted pawpaw    Plantae\n",
      "1  4755       Thomisus onustus  Heather crab spider  Arachnida\n",
      "2  3161                Nerodia          Watersnakes   Reptilia\n",
      "3  4493  Stagmomantis carolina      Carolina Mantis    Insecta\n",
      "4   757     Bridelia micrantha   Coastal Goldenleaf    Plantae\n",
      "\n",
      "Final data summary:\n",
      "Total records: 1899\n",
      "Records with 'Unknown' scientific_name: 0\n",
      "Records with 'Unknown' common_name: 0\n",
      "Records with 'Unknown' category: 0\n"
     ]
    }
   ],
   "source": [
    "# Clean the DataFrame\n",
    "if not nodes_df.empty:\n",
    "    print(\"Cleaning DataFrame...\")\n",
    "    \n",
    "    # Create a copy for cleaning\n",
    "    nodes_df_clean = nodes_df.copy()\n",
    "    \n",
    "    # Replace empty strings with NaN for better handling\n",
    "    nodes_df_clean = nodes_df_clean.replace('', pd.NA)\n",
    "    \n",
    "    # Fill missing values with appropriate defaults\n",
    "    nodes_df_clean['scientific_name'] = nodes_df_clean['scientific_name'].fillna('Unknown')\n",
    "    nodes_df_clean['common_name'] = nodes_df_clean['common_name'].fillna('Unknown')\n",
    "    nodes_df_clean['category'] = nodes_df_clean['category'].fillna('Unknown')\n",
    "    \n",
    "    # Ensure ID is string type\n",
    "    nodes_df_clean['ID'] = nodes_df_clean['ID'].astype(str)\n",
    "    \n",
    "    print(f\"Cleaned DataFrame shape: {nodes_df_clean.shape}\")\n",
    "    print(\"\\nCleaned data sample:\")\n",
    "    print(nodes_df_clean.head())\n",
    "    \n",
    "    # Final data summary\n",
    "    print(\"\\nFinal data summary:\")\n",
    "    print(f\"Total records: {len(nodes_df_clean)}\")\n",
    "    print(f\"Records with 'Unknown' scientific_name: {(nodes_df_clean['scientific_name'] == 'Unknown').sum()}\")\n",
    "    print(f\"Records with 'Unknown' common_name: {(nodes_df_clean['common_name'] == 'Unknown').sum()}\")\n",
    "    print(f\"Records with 'Unknown' category: {(nodes_df_clean['category'] == 'Unknown').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Statistics Summary:\n",
      "========================================\n",
      "Total nodes in network: 1899\n",
      "Total edges in network: 2313\n",
      "Records in DataFrame: 1899\n",
      "Network density: 0.0006\n",
      "\n",
      "Node degree statistics:\n",
      "  Average degree: 2.44\n",
      "  Max degree: 70\n",
      "  Min degree: 1\n",
      "\n",
      "Top 5 most connected nodes:\n",
      "  1. Lepidoptera (Butterflies and Moths) - Degree: 70\n",
      "  2. Apis mellifera (Western Honey Bee) - Degree: 51\n",
      "  3. Ardea herodias (Great Blue Heron) - Degree: 46\n",
      "  4. Pterygota (Winged and Once-winged Insects) - Degree: 32\n",
      "  5. Diptera (Flies) - Degree: 30\n"
     ]
    }
   ],
   "source": [
    "# Optional: Display network statistics\n",
    "if G is not None and not nodes_df.empty:\n",
    "    print(\"Network Statistics Summary:\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total nodes in network: {G.number_of_nodes()}\")\n",
    "    print(f\"Total edges in network: {G.number_of_edges()}\")\n",
    "    print(f\"Records in DataFrame: {len(nodes_df)}\")\n",
    "    print(f\"Network density: {nx.density(G):.4f}\")\n",
    "    \n",
    "    # Node degree statistics\n",
    "    degrees = [degree for node, degree in G.degree()]\n",
    "    print(f\"\\nNode degree statistics:\")\n",
    "    print(f\"  Average degree: {np.mean(degrees):.2f}\")\n",
    "    print(f\"  Max degree: {max(degrees)}\")\n",
    "    print(f\"  Min degree: {min(degrees)}\")\n",
    "    \n",
    "    # Show nodes with highest degree (most connected)\n",
    "    degree_dict = dict(G.degree())\n",
    "    sorted_degrees = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop 5 most connected nodes:\")\n",
    "    for i, (node_id, degree) in enumerate(sorted_degrees[:5], 1):\n",
    "        # Get node info from DataFrame\n",
    "        node_info = nodes_df[nodes_df['ID'] == node_id]\n",
    "        if not node_info.empty:\n",
    "            sci_name = node_info.iloc[0]['scientific_name']\n",
    "            common_name = node_info.iloc[0]['common_name']\n",
    "            print(f\"  {i}. {sci_name} ({common_name}) - Degree: {degree}\")\n",
    "        else:\n",
    "            print(f\"  {i}. Node {node_id} - Degree: {degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading animal features dataset...\n",
      "Successfully loaded animal features dataset with 3540 records\n",
      "Columns: ['scientific_name', 'common_name', 'weight', 'size', 'diet', 'life_span', 'habitat', 'continent']\n",
      "\n",
      "Animal features data sample:\n",
      "            scientific_name              common_name       diet  weight  size\n",
      "0  Abantennarius sanguineus          Bloody frogfish  Carnivore    50.0  10.0\n",
      "1         Abantis paradisea         Paradise Skipper  Herbivore     1.0   3.0\n",
      "2       Abbottina rivularis    Chinese false gudgeon   Omnivore    20.0  10.0\n",
      "3     Abisares viridipennis  African darkling beetle  Herbivore     5.0   1.0\n",
      "4             Abramis brama                    Bream   Omnivore  2000.0  50.0\n"
     ]
    }
   ],
   "source": [
    "# Load the animal features dataset for joining\n",
    "print(\"Loading animal features dataset...\")\n",
    "\n",
    "animal_features_file = '../data/combined_animal_features_final.csv'\n",
    "\n",
    "try:\n",
    "    animal_features_df = pd.read_csv(animal_features_file)\n",
    "    print(f\"Successfully loaded animal features dataset with {len(animal_features_df)} records\")\n",
    "    print(f\"Columns: {list(animal_features_df.columns)}\")\n",
    "    \n",
    "    # Convert habitat and continent columns back to lists if they're strings\n",
    "    if 'habitat' in animal_features_df.columns:\n",
    "        animal_features_df['habitat'] = animal_features_df['habitat'].apply(\n",
    "            lambda x: eval(x) if isinstance(x, str) and x.startswith('[') else x\n",
    "        )\n",
    "    if 'continent' in animal_features_df.columns:\n",
    "        animal_features_df['continent'] = animal_features_df['continent'].apply(\n",
    "            lambda x: eval(x) if isinstance(x, str) and x.startswith('[') else x\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nAnimal features data sample:\")\n",
    "    print(animal_features_df[['scientific_name', 'common_name', 'diet', 'weight', 'size']].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading animal features dataset: {e}\")\n",
    "    animal_features_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-join Analysis:\n",
      "==================================================\n",
      "Network nodes dataset: 1899 records\n",
      "Animal features dataset: 3540 records\n",
      "\n",
      "Network nodes - scientific_name analysis:\n",
      "  Valid scientific names: 1899\n",
      "  Unique scientific names: 1899\n",
      "  Sample names: ['Asimina reticulata', 'Thomisus onustus', 'Nerodia', 'Stagmomantis carolina', 'Bridelia micrantha']\n",
      "\n",
      "Animal features - scientific_name analysis:\n",
      "  Valid scientific names: 3540\n",
      "  Unique scientific names: 3540\n",
      "  Sample names: ['Abantennarius sanguineus', 'Abantis paradisea', 'Abbottina rivularis', 'Abisares viridipennis', 'Abramis brama']\n",
      "\n",
      "Potential matches (case-insensitive): 1254\n",
      "Sample potential matches: ['lycosidae', 'orphulella pelidna', 'pugettia producta', 'celastrina argiolus', 'taurulus bubalis', 'baeolophus bicolor', 'agulla', 'egretta garzetta', 'noctuoidea', 'tyto furcata']\n"
     ]
    }
   ],
   "source": [
    "# Analyze the datasets before joining\n",
    "if nodes_df_clean is not None and animal_features_df is not None:\n",
    "    print(\"Pre-join Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check data availability\n",
    "    print(f\"Network nodes dataset: {len(nodes_df_clean)} records\")\n",
    "    print(f\"Animal features dataset: {len(animal_features_df)} records\")\n",
    "    \n",
    "    # Check scientific_name field quality\n",
    "    print(f\"\\nNetwork nodes - scientific_name analysis:\")\n",
    "    network_sci_names = nodes_df_clean['scientific_name'].dropna()\n",
    "    network_sci_names = network_sci_names[network_sci_names != 'Unknown']\n",
    "    print(f\"  Valid scientific names: {len(network_sci_names)}\")\n",
    "    print(f\"  Unique scientific names: {len(network_sci_names.unique())}\")\n",
    "    print(f\"  Sample names: {list(network_sci_names.head())}\")\n",
    "    \n",
    "    print(f\"\\nAnimal features - scientific_name analysis:\")\n",
    "    features_sci_names = animal_features_df['scientific_name'].dropna()\n",
    "    print(f\"  Valid scientific names: {len(features_sci_names)}\")\n",
    "    print(f\"  Unique scientific names: {len(features_sci_names.unique())}\")\n",
    "    print(f\"  Sample names: {list(features_sci_names.head())}\")\n",
    "    \n",
    "    # Check for potential matches\n",
    "    network_names_set = set(network_sci_names.str.strip().str.lower())\n",
    "    features_names_set = set(features_sci_names.str.strip().str.lower())\n",
    "    \n",
    "    potential_matches = network_names_set.intersection(features_names_set)\n",
    "    print(f\"\\nPotential matches (case-insensitive): {len(potential_matches)}\")\n",
    "    \n",
    "    if len(potential_matches) > 0:\n",
    "        print(f\"Sample potential matches: {list(potential_matches)[:10]}\")\n",
    "    else:\n",
    "        print(\"No obvious matches found. Checking for partial matches...\")\n",
    "        # Show some examples from each dataset for manual inspection\n",
    "        print(f\"\\nNetwork scientific names sample: {list(network_sci_names.head(10))}\")\n",
    "        print(f\"Features scientific names sample: {list(features_sci_names.head(10))}\")\n",
    "else:\n",
    "    print(\"Cannot perform analysis - one or both datasets are not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing inner join on scientific_name...\n",
      "==================================================\n",
      "Network nodes available for joining: 1899\n",
      "Animal features records: 3540\n",
      "\n",
      "Inner join completed!\n",
      "Merged dataset shape: (1254, 12)\n",
      "Records successfully joined: 1254\n",
      "\n",
      "Merged dataset columns:\n",
      "   1. ID\n",
      "   2. scientific_name_network\n",
      "   3. common_name_network\n",
      "   4. category\n",
      "   5. scientific_name_features\n",
      "   6. common_name_features\n",
      "   7. weight\n",
      "   8. size\n",
      "   9. diet\n",
      "  10. life_span\n",
      "  11. habitat\n",
      "  12. continent\n",
      "\n",
      "Final merged dataset shape: (1254, 10)\n",
      "Final columns: ['ID', 'category', 'weight', 'size', 'diet', 'life_span', 'habitat', 'continent', 'scientific_name', 'common_name']\n"
     ]
    }
   ],
   "source": [
    "# Perform inner join on scientific_name\n",
    "if nodes_df_clean is not None and animal_features_df is not None:\n",
    "    print(\"Performing inner join on scientific_name...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Clean scientific names for better matching\n",
    "    # Create cleaned versions for joining\n",
    "    nodes_df_clean['scientific_name_clean'] = nodes_df_clean['scientific_name'].str.strip().str.lower()\n",
    "    animal_features_df['scientific_name_clean'] = animal_features_df['scientific_name'].str.strip().str.lower()\n",
    "    \n",
    "    # Remove records with Unknown or empty scientific names from network data\n",
    "    nodes_for_join = nodes_df_clean[\n",
    "        (nodes_df_clean['scientific_name'] != 'Unknown') & \n",
    "        (nodes_df_clean['scientific_name'].notna()) &\n",
    "        (nodes_df_clean['scientific_name'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Network nodes available for joining: {len(nodes_for_join)}\")\n",
    "    print(f\"Animal features records: {len(animal_features_df)}\")\n",
    "    \n",
    "    # Perform the inner join\n",
    "    merged_df = pd.merge(\n",
    "        nodes_for_join, \n",
    "        animal_features_df, \n",
    "        on='scientific_name_clean', \n",
    "        how='inner',\n",
    "        suffixes=('_network', '_features')\n",
    "    )\n",
    "    \n",
    "    # Drop the temporary clean column\n",
    "    merged_df = merged_df.drop('scientific_name_clean', axis=1)\n",
    "    \n",
    "    print(f\"\\nInner join completed!\")\n",
    "    print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "    print(f\"Records successfully joined: {len(merged_df)}\")\n",
    "    \n",
    "    if len(merged_df) > 0:\n",
    "        # Show column information\n",
    "        print(f\"\\nMerged dataset columns:\")\n",
    "        for i, col in enumerate(merged_df.columns, 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "        \n",
    "        # Handle duplicate columns (keep features version for scientific_name and common_name)\n",
    "        if 'scientific_name_network' in merged_df.columns and 'scientific_name_features' in merged_df.columns:\n",
    "            merged_df['scientific_name'] = merged_df['scientific_name_features']\n",
    "            merged_df = merged_df.drop(['scientific_name_network', 'scientific_name_features'], axis=1)\n",
    "        \n",
    "        if 'common_name_network' in merged_df.columns and 'common_name_features' in merged_df.columns:\n",
    "            merged_df['common_name'] = merged_df['common_name_features']\n",
    "            merged_df = merged_df.drop(['common_name_network', 'common_name_features'], axis=1)\n",
    "        \n",
    "        print(f\"\\nFinal merged dataset shape: {merged_df.shape}\")\n",
    "        print(f\"Final columns: {list(merged_df.columns)}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nWarning: No matches found between datasets!\")\n",
    "        merged_df = pd.DataFrame()\n",
    "else:\n",
    "    print(\"Cannot perform join - one or both datasets are not available\")\n",
    "    merged_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset Analysis:\n",
      "==================================================\n",
      "Final merged dataset info:\n",
      "  Shape: (1254, 10)\n",
      "  Records: 1254\n",
      "  Columns: 10\n",
      "\n",
      "Data types:\n",
      "ID                  object\n",
      "category            object\n",
      "weight             float64\n",
      "size               float64\n",
      "diet                object\n",
      "life_span          float64\n",
      "habitat             object\n",
      "continent           object\n",
      "scientific_name     object\n",
      "common_name         object\n",
      "dtype: object\n",
      "\n",
      "Sample of merged data:\n",
      "     ID        scientific_name                          common_name  \\\n",
      "0  4755       Thomisus onustus                   Flower Crab Spider   \n",
      "1  3161                Nerodia  North American Water Snakes (Genus)   \n",
      "2  4493  Stagmomantis carolina                      Carolina Mantis   \n",
      "3  1253     Copsychus saularis                Oriental Magpie-Robin   \n",
      "4   702        Bombus pratorum                      Early Bumblebee   \n",
      "\n",
      "    category         diet   weight   size  life_span  \n",
      "0  Arachnida  Insectivore    0.050    0.0       1.00  \n",
      "1   Reptilia    Carnivore  550.000  105.0       7.50  \n",
      "2    Insecta  Insectivore    2.000    5.0       0.75  \n",
      "3       Aves  Insectivore   35.000   21.0      11.50  \n",
      "4    Insecta    Herbivore    0.175    1.0       1.00  \n",
      "\n",
      "Missing values in merged dataset:\n",
      "  weight: 1 (0.1%)\n",
      "  life_span: 16 (1.3%)\n",
      "\n",
      "Diet distribution in merged dataset:\n",
      "diet\n",
      "Herbivore      416\n",
      "Omnivore       303\n",
      "Carnivore      300\n",
      "Insectivore    235\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Category distribution in merged dataset:\n",
      "category\n",
      "Insecta           544\n",
      "Aves              274\n",
      "Arachnida         111\n",
      "Actinopterygii     87\n",
      "Mammalia           81\n",
      "Animalia           59\n",
      "Reptilia           52\n",
      "Amphibia           25\n",
      "Mollusca           20\n",
      "Plantae             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset join statistics:\n",
      "  Network nodes with features: 1254/1899 (66.0%)\n",
      "  Feature records in network: 1254/3540 (35.4%)\n",
      "\n",
      "‚úÖ Merged dataset 'merged_df' is ready for use!\n",
      "   This dataset combines network topology with animal features.\n",
      "   Use this dataset for further analysis and visualization.\n"
     ]
    }
   ],
   "source": [
    "# Analyze the merged dataset\n",
    "if not merged_df.empty:\n",
    "    print(\"Merged Dataset Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Final merged dataset info:\")\n",
    "    print(f\"  Shape: {merged_df.shape}\")\n",
    "    print(f\"  Records: {len(merged_df)}\")\n",
    "    print(f\"  Columns: {len(merged_df.columns)}\")\n",
    "    \n",
    "    print(f\"\\nData types:\")\n",
    "    print(merged_df.dtypes)\n",
    "    \n",
    "    print(f\"\\nSample of merged data:\")\n",
    "    display_columns = ['ID', 'scientific_name', 'common_name', 'category', 'diet', 'weight', 'size', 'life_span']\n",
    "    available_columns = [col for col in display_columns if col in merged_df.columns]\n",
    "    print(merged_df[available_columns].head())\n",
    "    \n",
    "    print(f\"\\nMissing values in merged dataset:\")\n",
    "    missing_values = merged_df.isnull().sum()\n",
    "    for col, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count} ({count/len(merged_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Diet distribution in merged data\n",
    "    if 'diet' in merged_df.columns:\n",
    "        print(f\"\\nDiet distribution in merged dataset:\")\n",
    "        print(merged_df['diet'].value_counts())\n",
    "    \n",
    "    # Category distribution in merged data\n",
    "    if 'category' in merged_df.columns:\n",
    "        print(f\"\\nCategory distribution in merged dataset:\")\n",
    "        print(merged_df['category'].value_counts())\n",
    "    \n",
    "    # Network and features coverage\n",
    "    print(f\"\\nDataset join statistics:\")\n",
    "    if 'nodes_for_join' in locals():\n",
    "        coverage_network = len(merged_df) / len(nodes_for_join) * 100\n",
    "        print(f\"  Network nodes with features: {len(merged_df)}/{len(nodes_for_join)} ({coverage_network:.1f}%)\")\n",
    "    \n",
    "    if 'animal_features_df' in locals():\n",
    "        coverage_features = len(merged_df) / len(animal_features_df) * 100\n",
    "        print(f\"  Feature records in network: {len(merged_df)}/{len(animal_features_df)} ({coverage_features:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Merged dataset 'merged_df' is ready for use!\")\n",
    "    print(f\"   This dataset combines network topology with animal features.\")\n",
    "    print(f\"   Use this dataset for further analysis and visualization.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Merged dataset is empty - no matches found between network and features data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Statistics Summary:\n",
      "========================================\n",
      "Total nodes in network: 1899\n",
      "Total edges in network: 2313\n",
      "Network density: 0.0006\n",
      "\n",
      "Node degree statistics:\n",
      "  Average degree: 2.44\n",
      "  Max degree: 70\n",
      "  Min degree: 1\n",
      "\n",
      "Top 5 most connected nodes:\n",
      "  1. Lepidoptera (Butterflies and Moths) - Degree: 70\n",
      "  2. Apis mellifera (Western Honey Bee) - Degree: 51\n",
      "  3. Ardea herodias (Great Blue Heron) - Degree: 46\n",
      "  4. Pterygota (Winged and Once-winged Insects) - Degree: 32\n",
      "  5. Diptera (Flies) - Degree: 30\n",
      "\n",
      "Merged data coverage:\n",
      "  Nodes with both network and features data: 1254\n",
      "  Coverage of total network: 66.0%\n",
      "\n",
      "Top 5 most connected nodes WITH animal features:\n",
      "  1. Lepidoptera (Butterflies and Moths) - Degree: 70, Diet: Herbivore\n",
      "  2. Apis mellifera (European honey bee) - Degree: 51, Diet: Herbivore\n",
      "  3. Ardea herodias (Great Blue Heron) - Degree: 46, Diet: Carnivore\n",
      "  4. Pterygota (Winged Insects) - Degree: 32, Diet: Omnivore\n",
      "  5. Halcyon albiventris albiventris (Brown-hooded Kingfisher) - Degree: 22, Diet: Carnivore\n",
      "\n",
      "üéØ Network analysis complete! You now have:\n",
      "   - Network topology data (nodes_df_clean)\n",
      "   - Animal features data (animal_features_df)\n",
      "   - Combined data (merged_df) for network + features analysis\n"
     ]
    }
   ],
   "source": [
    "# Optional: Display network statistics with merged data context\n",
    "if G is not None and not nodes_df.empty:\n",
    "    print(\"Network Statistics Summary:\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total nodes in network: {G.number_of_nodes()}\")\n",
    "    print(f\"Total edges in network: {G.number_of_edges()}\")\n",
    "    print(f\"Network density: {nx.density(G):.4f}\")\n",
    "    \n",
    "    # Node degree statistics\n",
    "    degrees = [degree for node, degree in G.degree()]\n",
    "    print(f\"\\nNode degree statistics:\")\n",
    "    print(f\"  Average degree: {np.mean(degrees):.2f}\")\n",
    "    print(f\"  Max degree: {max(degrees)}\")\n",
    "    print(f\"  Min degree: {min(degrees)}\")\n",
    "    \n",
    "    # Show nodes with highest degree (most connected)\n",
    "    degree_dict = dict(G.degree())\n",
    "    sorted_degrees = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop 5 most connected nodes:\")\n",
    "    for i, (node_id, degree) in enumerate(sorted_degrees[:5], 1):\n",
    "        # Get node info from DataFrame\n",
    "        node_info = nodes_df[nodes_df['ID'] == node_id]\n",
    "        if not node_info.empty:\n",
    "            sci_name = node_info.iloc[0]['scientific_name']\n",
    "            common_name = node_info.iloc[0]['common_name']\n",
    "            print(f\"  {i}. {sci_name} ({common_name}) - Degree: {degree}\")\n",
    "        else:\n",
    "            print(f\"  {i}. Node {node_id} - Degree: {degree}\")\n",
    "    \n",
    "    # Show information about merged data coverage\n",
    "    if 'merged_df' in locals() and not merged_df.empty:\n",
    "        print(f\"\\nMerged data coverage:\")\n",
    "        print(f\"  Nodes with both network and features data: {len(merged_df)}\")\n",
    "        print(f\"  Coverage of total network: {len(merged_df)/G.number_of_nodes()*100:.1f}%\")\n",
    "        \n",
    "        # Show top connected nodes that have features\n",
    "        merged_node_ids = set(merged_df['ID'].astype(str))\n",
    "        connected_with_features = []\n",
    "        \n",
    "        for node_id, degree in sorted_degrees:\n",
    "            if str(node_id) in merged_node_ids:\n",
    "                connected_with_features.append((node_id, degree))\n",
    "                if len(connected_with_features) >= 5:\n",
    "                    break\n",
    "        \n",
    "        print(f\"\\nTop 5 most connected nodes WITH animal features:\")\n",
    "        for i, (node_id, degree) in enumerate(connected_with_features, 1):\n",
    "            node_info = merged_df[merged_df['ID'] == str(node_id)]\n",
    "            if not node_info.empty:\n",
    "                sci_name = node_info.iloc[0]['scientific_name']\n",
    "                common_name = node_info.iloc[0]['common_name']\n",
    "                diet = node_info.iloc[0].get('diet', 'Unknown')\n",
    "                print(f\"  {i}. {sci_name} ({common_name}) - Degree: {degree}, Diet: {diet}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Network analysis complete! You now have:\")\n",
    "    print(f\"   - Network topology data (nodes_df_clean)\")\n",
    "    print(f\"   - Animal features data (animal_features_df)\")\n",
    "    print(f\"   - Combined data (merged_df) for network + features analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Input Preparation\n",
    "\n",
    "Transform the merged dataset into GNN-ready format with proper feature engineering and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing GNN input features...\n",
      "============================================================\n",
      "Starting with 1254 records\n",
      "Required columns: ['scientific_name', 'weight', 'size', 'diet', 'life_span', 'habitat', 'continent']\n",
      "\n",
      "Data quality check:\n",
      "Missing values per column:\n",
      "  scientific_name: 0 (0.0%)\n",
      "  weight: 1 (0.1%)\n",
      "  size: 0 (0.0%)\n",
      "  diet: 0 (0.0%)\n",
      "  life_span: 16 (1.3%)\n",
      "  habitat: 0 (0.0%)\n",
      "  continent: 0 (0.0%)\n",
      "\n",
      "Removing records with missing critical values...\n",
      "Records after removing missing numerical values: 1237 (removed 17)\n",
      "\n",
      "Data units verification:\n",
      "  Weight: grams (confirmed from JSON processing)\n",
      "  Size: centimeters (confirmed from JSON processing)\n",
      "  Life span: years (confirmed from JSON processing)\n",
      "\n",
      "Numerical data ranges before normalization:\n",
      "  Weight: 1.00e-06 - 5.00e+07 grams\n",
      "  Size: 0.0 - 1575.0 cm\n",
      "  Life span: 0.003 - 110.0 years\n"
     ]
    }
   ],
   "source": [
    "# Prepare GNN input features from merged dataset\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "if not merged_df.empty:\n",
    "    print(\"Preparing GNN input features...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create a copy for GNN processing\n",
    "    gnn_df = merged_df.copy()\n",
    "    \n",
    "    # Select required columns for GNN\n",
    "    required_columns = ['scientific_name', 'weight', 'size', 'diet', 'life_span', 'habitat', 'continent']\n",
    "    \n",
    "    print(f\"Starting with {len(gnn_df)} records\")\n",
    "    print(f\"Required columns: {required_columns}\")\n",
    "    \n",
    "    # Check data quality before processing\n",
    "    print(f\"\\nData quality check:\")\n",
    "    print(f\"Missing values per column:\")\n",
    "    for col in required_columns:\n",
    "        if col in gnn_df.columns:\n",
    "            missing_count = gnn_df[col].isnull().sum()\n",
    "            print(f\"  {col}: {missing_count} ({missing_count/len(gnn_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Remove records with missing critical values\n",
    "    print(f\"\\nRemoving records with missing critical values...\")\n",
    "    original_count = len(gnn_df)\n",
    "    \n",
    "    # Remove records with missing weight, size, or life_span\n",
    "    gnn_df = gnn_df.dropna(subset=['weight', 'size', 'life_span'])\n",
    "    \n",
    "    print(f\"Records after removing missing numerical values: {len(gnn_df)} (removed {original_count - len(gnn_df)})\")\n",
    "    \n",
    "    # Verify units (from JSON processing notebook analysis)\n",
    "    print(f\"\\nData units verification:\")\n",
    "    print(f\"  Weight: grams (confirmed from JSON processing)\")\n",
    "    print(f\"  Size: centimeters (confirmed from JSON processing)\")\n",
    "    print(f\"  Life span: years (confirmed from JSON processing)\")\n",
    "    \n",
    "    print(f\"\\nNumerical data ranges before normalization:\")\n",
    "    print(f\"  Weight: {gnn_df['weight'].min():.2e} - {gnn_df['weight'].max():.2e} grams\")\n",
    "    print(f\"  Size: {gnn_df['size'].min():.1f} - {gnn_df['size'].max():.1f} cm\")\n",
    "    print(f\"  Life span: {gnn_df['life_span'].min():.3f} - {gnn_df['life_span'].max():.1f} years\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No merged dataset available for GNN processing\")\n",
    "    gnn_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing numerical features to [0,1]...\n",
      "==================================================\n",
      "Normalization completed!\n",
      "\n",
      "Normalized ranges (should all be 0.0 - 1.0):\n",
      "  Weight: 0.000 - 1.000\n",
      "  Size: 0.000 - 1.000\n",
      "  Life span: 0.000 - 1.000\n",
      "\n",
      "‚úÖ Scalers stored for potential inverse transformation\n",
      "\n",
      "Sample comparison (original vs normalized):\n",
      "    weight  weight_normalized   size  size_normalized  life_span  \\\n",
      "0    0.050       9.994803e-10    0.0         0.000000       1.00   \n",
      "1  550.000       1.099450e-05  105.0         0.066667       7.50   \n",
      "2    2.000       3.997999e-08    5.0         0.003175       0.75   \n",
      "3   35.000       6.996502e-07   21.0         0.013333      11.50   \n",
      "4    0.175       3.498231e-09    1.0         0.000635       1.00   \n",
      "\n",
      "   life_span_normalized  \n",
      "0              0.009066  \n",
      "1              0.068159  \n",
      "2              0.006793  \n",
      "3              0.104523  \n",
      "4              0.009066  \n"
     ]
    }
   ],
   "source": [
    "# Normalize numerical features (weight, size, life_span) to [0,1]\n",
    "if not gnn_df.empty:\n",
    "    print(\"Normalizing numerical features to [0,1]...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize scalers\n",
    "    weight_scaler = MinMaxScaler()\n",
    "    size_scaler = MinMaxScaler()\n",
    "    lifespan_scaler = MinMaxScaler()\n",
    "    \n",
    "    # Apply normalization\n",
    "    gnn_df['weight_normalized'] = weight_scaler.fit_transform(gnn_df[['weight']]).flatten()\n",
    "    gnn_df['size_normalized'] = size_scaler.fit_transform(gnn_df[['size']]).flatten()\n",
    "    gnn_df['life_span_normalized'] = lifespan_scaler.fit_transform(gnn_df[['life_span']]).flatten()\n",
    "    \n",
    "    print(f\"Normalization completed!\")\n",
    "    print(f\"\\nNormalized ranges (should all be 0.0 - 1.0):\")\n",
    "    print(f\"  Weight: {gnn_df['weight_normalized'].min():.3f} - {gnn_df['weight_normalized'].max():.3f}\")\n",
    "    print(f\"  Size: {gnn_df['size_normalized'].min():.3f} - {gnn_df['size_normalized'].max():.3f}\")\n",
    "    print(f\"  Life span: {gnn_df['life_span_normalized'].min():.3f} - {gnn_df['life_span_normalized'].max():.3f}\")\n",
    "    \n",
    "    # Store scalers for potential inverse transform later\n",
    "    scalers = {\n",
    "        'weight_scaler': weight_scaler,\n",
    "        'size_scaler': size_scaler,\n",
    "        'lifespan_scaler': lifespan_scaler\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Scalers stored for potential inverse transformation\")\n",
    "    \n",
    "    # Sample of normalized vs original values\n",
    "    print(f\"\\nSample comparison (original vs normalized):\")\n",
    "    comparison_cols = ['weight', 'weight_normalized', 'size', 'size_normalized', 'life_span', 'life_span_normalized']\n",
    "    print(gnn_df[comparison_cols].head())\n",
    "else:\n",
    "    print(\"‚ùå No data available for normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding diet features...\n",
      "==================================================\n",
      "Diet distribution:\n",
      "diet\n",
      "Herbivore      408\n",
      "Carnivore      299\n",
      "Omnivore       299\n",
      "Insectivore    231\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Diet one-hot encoded columns:\n",
      "  - diet_Carnivore: 299 records\n",
      "  - diet_Herbivore: 408 records\n",
      "  - diet_Insectivore: 231 records\n",
      "  - diet_Omnivore: 299 records\n",
      "\n",
      "Diet feature data types:\n",
      "  - diet_Carnivore: int64\n",
      "  - diet_Herbivore: int64\n",
      "  - diet_Insectivore: int64\n",
      "  - diet_Omnivore: int64\n",
      "\n",
      "‚úÖ Diet one-hot encoding completed!\n",
      "Added 4 diet feature columns (as integers)\n",
      "\n",
      "Sample diet encoding values:\n",
      "   diet_Carnivore  diet_Herbivore  diet_Insectivore  diet_Omnivore\n",
      "0               0               0                 1              0\n",
      "1               1               0                 0              0\n",
      "2               0               0                 1              0\n",
      "3               0               0                 1              0\n",
      "4               0               1                 0              0\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode diet features\n",
    "if not gnn_df.empty:\n",
    "    print(\"One-hot encoding diet features...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check diet distribution\n",
    "    print(f\"Diet distribution:\")\n",
    "    diet_counts = gnn_df['diet'].value_counts()\n",
    "    print(diet_counts)\n",
    "    \n",
    "    # Create one-hot encoding for diet (ensure integer output)\n",
    "    diet_dummies = pd.get_dummies(gnn_df['diet'], prefix='diet', dtype=int)\n",
    "    \n",
    "    print(f\"\\nDiet one-hot encoded columns:\")\n",
    "    for col in diet_dummies.columns:\n",
    "        print(f\"  - {col}: {diet_dummies[col].sum()} records\")\n",
    "    \n",
    "    # Verify data types are integers\n",
    "    print(f\"\\nDiet feature data types:\")\n",
    "    for col in diet_dummies.columns:\n",
    "        print(f\"  - {col}: {diet_dummies[col].dtype}\")\n",
    "    \n",
    "    # Add diet one-hot columns to dataframe\n",
    "    gnn_df = pd.concat([gnn_df, diet_dummies], axis=1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Diet one-hot encoding completed!\")\n",
    "    print(f\"Added {len(diet_dummies.columns)} diet feature columns (as integers)\")\n",
    "    \n",
    "    # Show sample values to confirm they're 0s and 1s\n",
    "    print(f\"\\nSample diet encoding values:\")\n",
    "    print(diet_dummies.head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for diet encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing habitat features for one-hot encoding...\n",
      "============================================================\n",
      "Total unique habitats: 311\n",
      "\n",
      "Top 10 most frequent habitats:\n",
      "   1. Forest: 610 records\n",
      "   2. Grassland: 400 records\n",
      "   3. Garden: 369 records\n",
      "   4. Urban: 345 records\n",
      "   5. Woodland: 326 records\n",
      "   6. Shrubland: 230 records\n",
      "   7. Wetland: 142 records\n",
      "   8. Savanna: 128 records\n",
      "   9. Meadow: 121 records\n",
      "  10. Gardens: 115 records\n",
      "\n",
      "Remaining habitats: 301\n",
      "Total 'other' habitat occurrences: 3023\n",
      "\n",
      "Habitat feature encoding summary:\n",
      "  - habitat_Forest: 610 records\n",
      "  - habitat_Grassland: 400 records\n",
      "  - habitat_Garden: 369 records\n",
      "  - habitat_Urban: 345 records\n",
      "  - habitat_Woodland: 326 records\n",
      "  - habitat_Shrubland: 230 records\n",
      "  - habitat_Wetland: 142 records\n",
      "  - habitat_Savanna: 128 records\n",
      "  - habitat_Meadow: 121 records\n",
      "  - habitat_Gardens: 115 records\n",
      "  - habitat_other: 1072 records\n",
      "\n",
      "Habitat feature data types:\n",
      "  - habitat_Forest: int64\n",
      "  - habitat_Grassland: int64\n",
      "  - habitat_Garden: int64\n",
      "  - habitat_Urban: int64\n",
      "  - habitat_Woodland: int64\n",
      "  - habitat_Shrubland: int64\n",
      "  - habitat_Wetland: int64\n",
      "  - habitat_Savanna: int64\n",
      "  - habitat_Meadow: int64\n",
      "  - habitat_Gardens: int64\n",
      "  - habitat_other: int64\n",
      "\n",
      "‚úÖ Habitat one-hot encoding completed!\n",
      "Added 11 habitat feature columns (10 top + 1 other, as integers)\n",
      "\n",
      "Sample habitat encoding values:\n",
      "   habitat_Forest  habitat_Grassland  habitat_Garden  habitat_Urban  \\\n",
      "0               0                  0               0              0   \n",
      "1               0                  0               0              0   \n",
      "2               1                  1               1              0   \n",
      "3               1                  0               1              1   \n",
      "4               0                  0               0              1   \n",
      "\n",
      "   habitat_Woodland  habitat_Shrubland  habitat_Wetland  habitat_Savanna  \\\n",
      "0                 0                  0                0                0   \n",
      "1                 0                  0                0                0   \n",
      "2                 1                  1                0                0   \n",
      "3                 1                  1                0                0   \n",
      "4                 0                  0                0                0   \n",
      "\n",
      "   habitat_Meadow  habitat_Gardens  habitat_other  \n",
      "0               1                0              0  \n",
      "1               0                0              1  \n",
      "2               0                0              0  \n",
      "3               0                0              0  \n",
      "4               0                1              1  \n"
     ]
    }
   ],
   "source": [
    "# Process habitat features - one-hot encode top 10 habitats + \"other\"\n",
    "if not gnn_df.empty:\n",
    "    print(\"Processing habitat features for one-hot encoding...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Count all habitat occurrences\n",
    "    habitat_counts = {}\n",
    "    for habitats in gnn_df['habitat']:\n",
    "        if isinstance(habitats, list):\n",
    "            for habitat in habitats:\n",
    "                habitat_counts[habitat] = habitat_counts.get(habitat, 0) + 1\n",
    "    \n",
    "    # Sort habitats by frequency and get top 10\n",
    "    sorted_habitats = sorted(habitat_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10_habitats = [habitat for habitat, count in sorted_habitats[:10]]\n",
    "    \n",
    "    print(f\"Total unique habitats: {len(habitat_counts)}\")\n",
    "    print(f\"\\nTop 10 most frequent habitats:\")\n",
    "    for i, (habitat, count) in enumerate(sorted_habitats[:10], 1):\n",
    "        print(f\"  {i:2d}. {habitat}: {count} records\")\n",
    "    \n",
    "    print(f\"\\nRemaining habitats: {len(sorted_habitats) - 10}\")\n",
    "    other_count = sum(count for habitat, count in sorted_habitats[10:])\n",
    "    print(f\"Total 'other' habitat occurrences: {other_count}\")\n",
    "    \n",
    "    # Create habitat encoding function\n",
    "    def encode_habitat_features(habitat_list):\n",
    "        \"\"\"Convert habitat list to one-hot encoding with top 10 + other\"\"\"\n",
    "        features = {f'habitat_{habitat}': 0 for habitat in top_10_habitats}\n",
    "        features['habitat_other'] = 0\n",
    "        \n",
    "        if isinstance(habitat_list, list):\n",
    "            for habitat in habitat_list:\n",
    "                if habitat in top_10_habitats:\n",
    "                    features[f'habitat_{habitat}'] = 1\n",
    "                else:\n",
    "                    features['habitat_other'] = 1\n",
    "        \n",
    "        return pd.Series(features, dtype=int)  # Ensure integer type\n",
    "    \n",
    "    # Apply habitat encoding\n",
    "    habitat_features = gnn_df['habitat'].apply(encode_habitat_features)\n",
    "    \n",
    "    print(f\"\\nHabitat feature encoding summary:\")\n",
    "    for col in habitat_features.columns:\n",
    "        count = habitat_features[col].sum()\n",
    "        print(f\"  - {col}: {count} records\")\n",
    "    \n",
    "    # Verify data types are integers\n",
    "    print(f\"\\nHabitat feature data types:\")\n",
    "    for col in habitat_features.columns:\n",
    "        print(f\"  - {col}: {habitat_features[col].dtype}\")\n",
    "    \n",
    "    # Add habitat features to dataframe\n",
    "    gnn_df = pd.concat([gnn_df, habitat_features], axis=1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Habitat one-hot encoding completed!\")\n",
    "    print(f\"Added {len(habitat_features.columns)} habitat feature columns (10 top + 1 other, as integers)\")\n",
    "    \n",
    "    # Show sample values to confirm they're 0s and 1s\n",
    "    print(f\"\\nSample habitat encoding values:\")\n",
    "    print(habitat_features.head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for habitat encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding continent features...\n",
      "==================================================\n",
      "Continent distribution:\n",
      "  - North America: 791 records\n",
      "  - Africa: 595 records\n",
      "  - Asia: 537 records\n",
      "  - Europe: 471 records\n",
      "  - South America: 310 records\n",
      "  - Oceania: 276 records\n",
      "  - Central America: 161 records\n",
      "\n",
      "Continent feature encoding summary:\n",
      "  - continent_Europe: 471 records\n",
      "  - continent_Asia: 537 records\n",
      "  - continent_Africa: 595 records\n",
      "  - continent_North_America: 791 records\n",
      "  - continent_Central_America: 161 records\n",
      "  - continent_South_America: 310 records\n",
      "  - continent_Oceania: 276 records\n",
      "\n",
      "Continent feature data types:\n",
      "  - continent_Europe: int64\n",
      "  - continent_Asia: int64\n",
      "  - continent_Africa: int64\n",
      "  - continent_North_America: int64\n",
      "  - continent_Central_America: int64\n",
      "  - continent_South_America: int64\n",
      "  - continent_Oceania: int64\n",
      "\n",
      "‚úÖ Continent one-hot encoding completed!\n",
      "Added 7 continent feature columns (as integers)\n",
      "\n",
      "Sample continent encoding values:\n",
      "   continent_Europe  continent_Asia  continent_Africa  \\\n",
      "0                 1               1                 1   \n",
      "1                 0               0                 0   \n",
      "2                 0               0                 0   \n",
      "3                 0               1                 0   \n",
      "4                 1               1                 0   \n",
      "\n",
      "   continent_North_America  continent_Central_America  \\\n",
      "0                        0                          0   \n",
      "1                        1                          0   \n",
      "2                        1                          1   \n",
      "3                        0                          0   \n",
      "4                        0                          0   \n",
      "\n",
      "   continent_South_America  continent_Oceania  \n",
      "0                        0                  0  \n",
      "1                        0                  0  \n",
      "2                        0                  0  \n",
      "3                        0                  0  \n",
      "4                        0                  0  \n"
     ]
    }
   ],
   "source": [
    "# One-hot encode continent features\n",
    "if not gnn_df.empty:\n",
    "    print(\"One-hot encoding continent features...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Count continent occurrences\n",
    "    continent_counts = {}\n",
    "    for continents in gnn_df['continent']:\n",
    "        if isinstance(continents, list):\n",
    "            for continent in continents:\n",
    "                continent_counts[continent] = continent_counts.get(continent, 0) + 1\n",
    "    \n",
    "    print(f\"Continent distribution:\")\n",
    "    sorted_continents = sorted(continent_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    for continent, count in sorted_continents:\n",
    "        print(f\"  - {continent}: {count} records\")\n",
    "    \n",
    "    # Get all unique continents for encoding\n",
    "    all_continents = list(continent_counts.keys())\n",
    "    \n",
    "    # Create continent encoding function\n",
    "    def encode_continent_features(continent_list):\n",
    "        \"\"\"Convert continent list to one-hot encoding\"\"\"\n",
    "        features = {f'continent_{continent.replace(\" \", \"_\")}': 0 for continent in all_continents}\n",
    "        \n",
    "        if isinstance(continent_list, list):\n",
    "            for continent in continent_list:\n",
    "                if continent in all_continents:\n",
    "                    features[f'continent_{continent.replace(\" \", \"_\")}'] = 1\n",
    "        \n",
    "        return pd.Series(features, dtype=int)  # Ensure integer type\n",
    "    \n",
    "    # Apply continent encoding\n",
    "    continent_features = gnn_df['continent'].apply(encode_continent_features)\n",
    "    \n",
    "    print(f\"\\nContinent feature encoding summary:\")\n",
    "    for col in continent_features.columns:\n",
    "        count = continent_features[col].sum()\n",
    "        print(f\"  - {col}: {count} records\")\n",
    "    \n",
    "    # Verify data types are integers\n",
    "    print(f\"\\nContinent feature data types:\")\n",
    "    for col in continent_features.columns:\n",
    "        print(f\"  - {col}: {continent_features[col].dtype}\")\n",
    "    \n",
    "    # Add continent features to dataframe\n",
    "    gnn_df = pd.concat([gnn_df, continent_features], axis=1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Continent one-hot encoding completed!\")\n",
    "    print(f\"Added {len(continent_features.columns)} continent feature columns (as integers)\")\n",
    "    \n",
    "    # Show sample values to confirm they're 0s and 1s\n",
    "    print(f\"\\nSample continent encoding values:\")\n",
    "    print(continent_features.head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for continent encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final GNN input dataframe...\n",
      "============================================================\n",
      "Final GNN input dataframe created!\n",
      "Shape: (1237, 26)\n",
      "Records: 1237\n",
      "Features: 25\n",
      "\n",
      "Feature breakdown:\n",
      "  - Numerical features (normalized): 3\n",
      "    * weight_normalized, size_normalized, life_span_normalized\n",
      "  - Diet features (one-hot): 4\n",
      "    * diet_Carnivore, diet_Herbivore, diet_Insectivore, diet_Omnivore\n",
      "  - Habitat features (one-hot): 11\n",
      "    * Top 10 + other habitat categories\n",
      "  - Continent features (one-hot): 7\n",
      "    * continent_Europe, continent_Asia, continent_Africa, continent_North_America, continent_Central_America, continent_South_America, continent_Oceania\n",
      "\n",
      "Data quality check:\n",
      "Missing values in final dataframe:\n",
      "  ‚úÖ No missing values in final dataframe!\n",
      "\n",
      "Sample of final GNN input dataframe:\n",
      "Columns: ['scientific_name', 'weight_normalized', 'size_normalized', 'life_span_normalized', 'diet_Carnivore', 'diet_Herbivore', 'diet_Insectivore', 'diet_Omnivore', 'habitat_Forest', 'habitat_Grassland', 'habitat_Garden', 'habitat_Urban', 'habitat_Woodland', 'habitat_Shrubland', 'habitat_Wetland', 'habitat_Savanna', 'habitat_Meadow', 'habitat_Gardens', 'habitat_other', 'continent_Europe', 'continent_Asia', 'continent_Africa', 'continent_North_America', 'continent_Central_America', 'continent_South_America', 'continent_Oceania']\n",
      "\n",
      "First 3 rows (showing first 10 columns):\n",
      "         scientific_name  weight_normalized  size_normalized  \\\n",
      "0       Thomisus onustus       9.994803e-10         0.000000   \n",
      "1                Nerodia       1.099450e-05         0.066667   \n",
      "2  Stagmomantis carolina       3.997999e-08         0.003175   \n",
      "\n",
      "   life_span_normalized  diet_Carnivore  diet_Herbivore  diet_Insectivore  \\\n",
      "0              0.009066               0               0                 1   \n",
      "1              0.068159               1               0                 0   \n",
      "2              0.006793               0               0                 1   \n",
      "\n",
      "   diet_Omnivore  habitat_Forest  habitat_Grassland  \n",
      "0              0               0                  0  \n",
      "1              0               0                  0  \n",
      "2              0               1                  1  \n",
      "\n",
      "üéØ GNN input dataframe is ready!\n",
      "   Variable: 'gnn_input_df'\n",
      "   Shape: (1237, 26)\n",
      "   Features: 25 (ready for GNN input)\n"
     ]
    }
   ],
   "source": [
    "# Create final GNN input dataframe\n",
    "if not gnn_df.empty:\n",
    "    print(\"Creating final GNN input dataframe...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define the columns for the final GNN input\n",
    "    # Original fields\n",
    "    base_columns = ['scientific_name']\n",
    "    \n",
    "    # Normalized numerical features\n",
    "    numerical_columns = ['weight_normalized', 'size_normalized', 'life_span_normalized']\n",
    "    \n",
    "    # One-hot encoded categorical features\n",
    "    diet_columns = [col for col in gnn_df.columns if col.startswith('diet_')]\n",
    "    habitat_columns = [col for col in gnn_df.columns if col.startswith('habitat_')]\n",
    "    continent_columns = [col for col in gnn_df.columns if col.startswith('continent_')]\n",
    "    \n",
    "    # Combine all feature columns\n",
    "    feature_columns = numerical_columns + diet_columns + habitat_columns + continent_columns\n",
    "    final_columns = base_columns + feature_columns\n",
    "    \n",
    "    # Create final GNN dataframe\n",
    "    gnn_input_df = gnn_df[final_columns].copy()\n",
    "    \n",
    "    print(f\"Final GNN input dataframe created!\")\n",
    "    print(f\"Shape: {gnn_input_df.shape}\")\n",
    "    print(f\"Records: {len(gnn_input_df)}\")\n",
    "    print(f\"Features: {len(feature_columns)}\")\n",
    "    \n",
    "    print(f\"\\nFeature breakdown:\")\n",
    "    print(f\"  - Numerical features (normalized): {len(numerical_columns)}\")\n",
    "    print(f\"    * {', '.join(numerical_columns)}\")\n",
    "    print(f\"  - Diet features (one-hot): {len(diet_columns)}\")\n",
    "    print(f\"    * {', '.join(diet_columns)}\")\n",
    "    print(f\"  - Habitat features (one-hot): {len(habitat_columns)}\")\n",
    "    print(f\"    * Top 10 + other habitat categories\")\n",
    "    print(f\"  - Continent features (one-hot): {len(continent_columns)}\")\n",
    "    print(f\"    * {', '.join(continent_columns)}\")\n",
    "    \n",
    "    # Data quality check\n",
    "    print(f\"\\nData quality check:\")\n",
    "    print(f\"Missing values in final dataframe:\")\n",
    "    missing_check = gnn_input_df.isnull().sum()\n",
    "    for col, missing_count in missing_check.items():\n",
    "        if missing_count > 0:\n",
    "            print(f\"  - {col}: {missing_count}\")\n",
    "    \n",
    "    if missing_check.sum() == 0:\n",
    "        print(f\"  ‚úÖ No missing values in final dataframe!\")\n",
    "    \n",
    "    # Show sample of final dataframe\n",
    "    print(f\"\\nSample of final GNN input dataframe:\")\n",
    "    print(\"Columns:\", list(gnn_input_df.columns))\n",
    "    print(\"\\nFirst 3 rows (showing first 10 columns):\")\n",
    "    sample_cols = final_columns[:min(10, len(final_columns))]\n",
    "    print(gnn_input_df[sample_cols].head(3))\n",
    "    \n",
    "    print(f\"\\nüéØ GNN input dataframe is ready!\")\n",
    "    print(f\"   Variable: 'gnn_input_df'\")\n",
    "    print(f\"   Shape: {gnn_input_df.shape}\")\n",
    "    print(f\"   Features: {len(feature_columns)} (ready for GNN input)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot create GNN input - no processed data available\")\n",
    "    gnn_input_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>weight_normalized</th>\n",
       "      <th>size_normalized</th>\n",
       "      <th>life_span_normalized</th>\n",
       "      <th>diet_Carnivore</th>\n",
       "      <th>diet_Herbivore</th>\n",
       "      <th>diet_Insectivore</th>\n",
       "      <th>diet_Omnivore</th>\n",
       "      <th>habitat_Forest</th>\n",
       "      <th>habitat_Grassland</th>\n",
       "      <th>...</th>\n",
       "      <th>habitat_Meadow</th>\n",
       "      <th>habitat_Gardens</th>\n",
       "      <th>habitat_other</th>\n",
       "      <th>continent_Europe</th>\n",
       "      <th>continent_Asia</th>\n",
       "      <th>continent_Africa</th>\n",
       "      <th>continent_North_America</th>\n",
       "      <th>continent_Central_America</th>\n",
       "      <th>continent_South_America</th>\n",
       "      <th>continent_Oceania</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomisus onustus</td>\n",
       "      <td>9.994803e-10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nerodia</td>\n",
       "      <td>1.099450e-05</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.068159</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stagmomantis carolina</td>\n",
       "      <td>3.997999e-08</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Copsychus saularis</td>\n",
       "      <td>6.996502e-07</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.104523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bombus pratorum</td>\n",
       "      <td>3.498231e-09</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         scientific_name  weight_normalized  size_normalized  \\\n",
       "0       Thomisus onustus       9.994803e-10         0.000000   \n",
       "1                Nerodia       1.099450e-05         0.066667   \n",
       "2  Stagmomantis carolina       3.997999e-08         0.003175   \n",
       "3     Copsychus saularis       6.996502e-07         0.013333   \n",
       "4        Bombus pratorum       3.498231e-09         0.000635   \n",
       "\n",
       "   life_span_normalized  diet_Carnivore  diet_Herbivore  diet_Insectivore  \\\n",
       "0              0.009066               0               0                 1   \n",
       "1              0.068159               1               0                 0   \n",
       "2              0.006793               0               0                 1   \n",
       "3              0.104523               0               0                 1   \n",
       "4              0.009066               0               1                 0   \n",
       "\n",
       "   diet_Omnivore  habitat_Forest  habitat_Grassland  ...  habitat_Meadow  \\\n",
       "0              0               0                  0  ...               1   \n",
       "1              0               0                  0  ...               0   \n",
       "2              0               1                  1  ...               0   \n",
       "3              0               1                  0  ...               0   \n",
       "4              0               0                  0  ...               0   \n",
       "\n",
       "   habitat_Gardens  habitat_other  continent_Europe  continent_Asia  \\\n",
       "0                0              0                 1               1   \n",
       "1                0              1                 0               0   \n",
       "2                0              0                 0               0   \n",
       "3                0              0                 0               1   \n",
       "4                1              1                 1               1   \n",
       "\n",
       "   continent_Africa  continent_North_America  continent_Central_America  \\\n",
       "0                 1                        0                          0   \n",
       "1                 0                        1                          0   \n",
       "2                 0                        1                          1   \n",
       "3                 0                        0                          0   \n",
       "4                 0                        0                          0   \n",
       "\n",
       "   continent_South_America  continent_Oceania  \n",
       "0                        0                  0  \n",
       "1                        0                  0  \n",
       "2                        0                  0  \n",
       "3                        0                  0  \n",
       "4                        0                  0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the dataframe to a csv file\n",
    "gnn_input_df.to_csv(\"../data/animals_transformed.csv\", index=False)\n",
    "\n",
    "gnn_input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
